{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHcTRGvUmoME"
      },
      "source": [
        "# Dance Diffusion v0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u97w34BXmust"
      },
      "source": [
        "Licensed under the MIT License\n",
        "\n",
        "Copyright (c) 2022 Zach Evans\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in\n",
        "all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "THE SOFTWARE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU97ZiP7nSKS"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxb-qgh0nUOf",
        "outputId": "79714f74-3c77-48d8-e7bc-90466fec5270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-c284e20c-e4b9-6985-cae2-1df22ca2c888)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Check GPU Status\n",
        "import subprocess\n",
        "simple_nvidia_smi_display = True#@param {type:\"boolean\"}\n",
        "if simple_nvidia_smi_display:\n",
        "    #!nvidia-smi\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "else:\n",
        "    #!nvidia-smi -i 0 -e 0\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "    nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_ecc_note)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_mFtzHvnlJL",
        "outputId": "f2e5d097-4266-4753-a81c-89ae83c25a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google Colab detected. Using Google Drive.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title 1.2 Prepare Folders\n",
        "import subprocess, os, sys, ipykernel\n",
        "\n",
        "def gitclone(url, targetdir=None):\n",
        "    if targetdir:\n",
        "        res = subprocess.run(['git', 'clone', url, targetdir], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    else:\n",
        "        res = subprocess.run(['git', 'clone', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def pipi(modulestr):\n",
        "    res = subprocess.run(['pip', 'install', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def pipie(modulestr):\n",
        "    res = subprocess.run(['git', 'install', '-e', modulestr], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def wget(url, outputdir):\n",
        "    res = subprocess.run(['wget', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"Google Colab detected. Using Google Drive.\")\n",
        "    is_colab = True\n",
        "    #@markdown If you connect your Google Drive, you can save the final image of each run on your drive.\n",
        "    google_drive = True #@param {type:\"boolean\"}\n",
        "    #@markdown Click here if you'd like to save the diffusion model checkpoint file to (and/or load from) your Google Drive:\n",
        "    save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
        "except:\n",
        "    is_colab = False\n",
        "    google_drive = False\n",
        "    save_models_to_google_drive = False\n",
        "    print(\"Google Colab not detected.\")\n",
        "\n",
        "if is_colab:\n",
        "    if google_drive is True:\n",
        "        drive.mount('/content/drive')\n",
        "        root_path = '/content/drive/MyDrive/AI/Bass_Diffusion'\n",
        "    else:\n",
        "        root_path = '/content'\n",
        "else:\n",
        "    root_path = os.getcwd()\n",
        "\n",
        "import os\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "initDirPath = f'{root_path}/init_audio'\n",
        "createPath(initDirPath)\n",
        "outDirPath = f'{root_path}/audio_out'\n",
        "createPath(outDirPath)\n",
        "\n",
        "if is_colab:\n",
        "    if google_drive and not save_models_to_google_drive or not google_drive:\n",
        "        model_path = '/content/models'\n",
        "        createPath(model_path)\n",
        "    if google_drive and save_models_to_google_drive:\n",
        "        model_path = f'{root_path}/models'\n",
        "        createPath(model_path)\n",
        "else:\n",
        "    model_path = f'{root_path}/models'\n",
        "    createPath(model_path)\n",
        "\n",
        "# libraries = f'{root_path}/libraries'\n",
        "# createPath(libraries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9BS0ks1oEgP",
        "outputId": "16775bd0-f79e-42aa-9a8e-4edba20698c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'sample-generator'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 24 (delta 5), reused 13 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/harmonai-org/sample-generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhK7OgrToteL"
      },
      "outputs": [],
      "source": [
        "!pip install /content/sample-generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vcb_D9-4qGbj"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "from prefigure.prefigure import get_all_args\n",
        "from contextlib import contextmanager\n",
        "from copy import deepcopy\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "from tqdm import trange\n",
        "from einops import rearrange\n",
        "\n",
        "import torchaudio\n",
        "from audio_diffusion.models import DiffusionAttnUnet1D\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myZ7zb6cqbd8"
      },
      "outputs": [],
      "source": [
        "#@title Model code\n",
        "class DiffusionUncond(nn.Module):\n",
        "    def __init__(self, global_args):\n",
        "        super().__init__()\n",
        "\n",
        "        self.diffusion = DiffusionAttnUnet1D(global_args, n_attn_layers=4)\n",
        "        self.diffusion_ema = deepcopy(self.diffusion)\n",
        "        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmx5uHCNql4B"
      },
      "outputs": [],
      "source": [
        "#@title Sampling code\n",
        "# Define the noise schedule and sampling loop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "def plot_and_hear(audio):\n",
        "    display(ipd.Audio(audio.cpu(), rate=48000))\n",
        "    plt.plot(audio.cpu().t().numpy())\n",
        "    \n",
        "def get_alphas_sigmas(t):\n",
        "    \"\"\"Returns the scaling factors for the clean image (alpha) and for the\n",
        "    noise (sigma), given a timestep.\"\"\"\n",
        "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
        "\n",
        "def get_crash_schedule(t):\n",
        "    sigma = torch.sin(t * math.pi / 2) ** 2\n",
        "    alpha = (1 - sigma ** 2) ** 0.5\n",
        "    return alpha_sigma_to_t(alpha, sigma)\n",
        "\n",
        "def t_to_alpha_sigma(t):\n",
        "    \"\"\"Returns the scaling factors for the clean image and for the noise, given\n",
        "    a timestep.\"\"\"\n",
        "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
        "\n",
        "def alpha_sigma_to_t(alpha, sigma):\n",
        "    \"\"\"Returns a timestep, given the scaling factors for the clean image and for\n",
        "    the noise.\"\"\"\n",
        "    return torch.atan2(sigma, alpha) / math.pi * 2\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, x, steps, eta):\n",
        "    \"\"\"Draws samples from a model given starting noise.\"\"\"\n",
        "    ts = x.new_ones([x.shape[0]])\n",
        "\n",
        "    # Create the noise schedule\n",
        "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
        "    alphas, sigmas = get_alphas_sigmas(get_crash_schedule(t))\n",
        "\n",
        "    # The sampling loop\n",
        "    for i in trange(steps):\n",
        "\n",
        "        # Get the model output (v, the predicted velocity)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            v = model(x, ts * t[i]).float()\n",
        "\n",
        "        # Predict the noise and the denoised image\n",
        "        pred = x * alphas[i] - v * sigmas[i]\n",
        "        eps = x * sigmas[i] + v * alphas[i]\n",
        "\n",
        "        # If we are not on the last timestep, compute the noisy image for the\n",
        "        # next timestep.\n",
        "        if i < steps - 1:\n",
        "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
        "            # downward according to the amount of additional noise to add\n",
        "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
        "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
        "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "            # Recombine the predicted noise and predicted denoised image in the\n",
        "            # correct proportions for the next step\n",
        "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
        "\n",
        "            # Add the correct amount of fresh noise\n",
        "            if eta:\n",
        "                x += torch.randn_like(x) * ddim_sigma\n",
        "\n",
        "    # If we are on the last timestep, output the denoised image\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wvj3vrvrOKK"
      },
      "outputs": [],
      "source": [
        "#@title Args\n",
        "\n",
        "# Number of samples to train on must be a multiple of 16384\n",
        "sample_size = 65536 \n",
        "\n",
        "# the random seed\n",
        "seed = 42\n",
        "\n",
        "# The sample rate of the audio\n",
        "sample_rate = 48000   \n",
        "\n",
        "# the validation set\n",
        "latent_dim = 0             \n",
        "\n",
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "args = Object()\n",
        "args.sample_size = sample_size\n",
        "args.sample_rate = sample_rate\n",
        "args.latent_dim = latent_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHsHQcc6rHu7"
      },
      "outputs": [],
      "source": [
        "#@title Create the model\n",
        "ckpt_path = \"\" #@param {type:\"string\"}\n",
        "model = DiffusionUncond(args)\n",
        "model.load_state_dict(torch.load(ckpt_path)[\"model\"])\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.requires_grad_(False).to(device)\n",
        "\n",
        "# Remove non-EMA\n",
        "del model.diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GQK9yZHTr_z"
      },
      "source": [
        "# Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zntGqLTJq6xU"
      },
      "outputs": [],
      "source": [
        "#@markdown Diffusion options\n",
        "batch_size = 16 #@param {type:\"number\"}\n",
        "diffusion_steps =  500#@param {type:\"number\"}\n",
        "\n",
        "model_fn = model.diffusion_ema\n",
        "\n",
        "noise = torch.randn([batch_size, 2, args.sample_size]).to(device)\n",
        "\n",
        "generated = sample(model_fn, noise, diffusion_steps, 0)\n",
        "\n",
        "# Hard-clip the generated audio\n",
        "generated = generated.clamp(-1, 1)\n",
        "\n",
        "# Put the demos together\n",
        "generated = rearrange(generated, 'b d n -> d (b n)')\n",
        "\n",
        "plot_and_hear(generated)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HHcTRGvUmoME"
      ],
      "name": "Dance Diffusion.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
